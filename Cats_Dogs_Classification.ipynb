{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Введение"
      ],
      "metadata": {
        "id": "yEJbtGuHCYRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном проекте реализована задача классификации изображений с двумя классами: кошки и собаки.  \n",
        "\n",
        "Целью данного проекта является создание и обучение нейросетевой модели, способной с высокой точностью различать изображения кошек и собак. Для решения этой задачи используется предобученная модель MobileNet, которая была модифицирована под данную задачу с учетом специфики предоставленного набора данных\n",
        "\n",
        "Для обучения модели используется популярный датасет \"Кошки и Собаки\" (Cats and Dogs), который содержит тысячи изображений двух классов: кошек и собак"
      ],
      "metadata": {
        "id": "S4Do10QmCcJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "VJu4pvOADfNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import MobileNet\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input"
      ],
      "metadata": {
        "id": "0A-TTjLRDhH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка и распаковка датасета"
      ],
      "metadata": {
        "id": "LfiIGKYyBL74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\n",
        "!unzip -qo \"cat-and-dog\" -d ./temp\n",
        "\n",
        "IMAGE_PATH = './temp/training_set/training_set/'\n",
        "BASE_DIR = './dataset/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qvR86XfBHQO",
        "outputId": "2d61e1a4-5e67-40ba-fa9f-89ca755eb8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-09 20:57:41--  https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228082266 (218M) [application/x-zip-compressed]\n",
            "Saving to: ‘cat-and-dog.zip’\n",
            "\n",
            "cat-and-dog.zip     100%[===================>] 217.52M  15.1MB/s    in 16s     \n",
            "\n",
            "2024-10-09 20:57:59 (13.2 MB/s) - ‘cat-and-dog.zip’ saved [228082266/228082266]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-val-test"
      ],
      "metadata": {
        "id": "JCT9JkLYBTph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
        "CLASS_COUNT = len(CLASS_LIST)\n",
        "\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "\n",
        "os.mkdir(BASE_DIR)\n",
        "train_dir = os.path.join(BASE_DIR, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(BASE_DIR, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(BASE_DIR, 'test')\n",
        "os.mkdir(test_dir)"
      ],
      "metadata": {
        "id": "636WzjDFBRhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Каталог для датасета"
      ],
      "metadata": {
        "id": "zi6DP6hvBkog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(img_path, new_path, class_name, start_index, end_index):\n",
        "    src_path = os.path.join(img_path, class_name)\n",
        "    dst_path = os.path.join(new_path, class_name)\n",
        "    class_files = os.listdir(src_path)\n",
        "    os.mkdir(dst_path)\n",
        "    for fname in class_files[start_index:end_index]:\n",
        "        src = os.path.join(src_path, fname)\n",
        "        dst = os.path.join(dst_path, fname)\n",
        "        shutil.copyfile(src, dst)"
      ],
      "metadata": {
        "id": "u2XJQ73ABbMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_label in range(CLASS_COUNT):\n",
        "    class_name = CLASS_LIST[class_label]\n",
        "    create_dataset(IMAGE_PATH, train_dir, class_name, 0, 2000)\n",
        "    create_dataset(IMAGE_PATH, validation_dir, class_name, 2000, 3000)\n",
        "    create_dataset(IMAGE_PATH, test_dir, class_name, 3000, 4000)"
      ],
      "metadata": {
        "id": "et5RlYjYBdCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генерация данных"
      ],
      "metadata": {
        "id": "n-aKlh2LBzhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPhrh1mMBwS7",
        "outputId": "eeff7d48-0ec6-4ccf-813a-6e6514253103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Константы"
      ],
      "metadata": {
        "id": "gMSQugxGB9_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_WIDTH = 150\n",
        "IMG_HEIGHT = 150\n",
        "NUM_CLASSES = CLASS_COUNT"
      ],
      "metadata": {
        "id": "1BcUJgUrB7o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание кастомной модели на основе архитектуры MobileNet"
      ],
      "metadata": {
        "id": "GHxfePloCCLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_maker():\n",
        "    base_model = MobileNet(include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "\n",
        "    return Model(inputs=input, outputs=predictions)\n",
        "\n",
        "model = model_maker()\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErX83RMNB_ub",
        "outputId": "f7ce772b-35cc-40b4-a79d-56e1a08c5c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e222298ede81>:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNet(include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "CNjElFhECLPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4QBf2a2CKzh",
        "outputId": "4cd92bfa-820a-4e09-c011-ff51889d33b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 257ms/step - accuracy: 0.8785 - loss: 0.2634 - val_accuracy: 0.9581 - val_loss: 0.1071\n",
            "Epoch 2/30\n",
            "\u001b[1m 24/100\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.9294 - loss: 0.1817"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9308 - loss: 0.1828 - val_accuracy: 0.9600 - val_loss: 0.0844\n",
            "Epoch 3/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 221ms/step - accuracy: 0.9346 - loss: 0.1624 - val_accuracy: 0.9538 - val_loss: 0.1127\n",
            "Epoch 4/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9272 - loss: 0.1693 - val_accuracy: 0.9625 - val_loss: 0.0846\n",
            "Epoch 5/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 232ms/step - accuracy: 0.9245 - loss: 0.1752 - val_accuracy: 0.9663 - val_loss: 0.0811\n",
            "Epoch 6/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.9365 - loss: 0.1439 - val_accuracy: 0.9850 - val_loss: 0.0505\n",
            "Epoch 7/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 230ms/step - accuracy: 0.9343 - loss: 0.1633 - val_accuracy: 0.9669 - val_loss: 0.0755\n",
            "Epoch 8/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.9378 - loss: 0.1645 - val_accuracy: 0.9575 - val_loss: 0.0969\n",
            "Epoch 9/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 230ms/step - accuracy: 0.9345 - loss: 0.1555 - val_accuracy: 0.9675 - val_loss: 0.0794\n",
            "Epoch 10/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9339 - loss: 0.1438 - val_accuracy: 0.9725 - val_loss: 0.0636\n",
            "Epoch 11/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 223ms/step - accuracy: 0.9361 - loss: 0.1493 - val_accuracy: 0.9681 - val_loss: 0.0744\n",
            "Epoch 12/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9509 - loss: 0.1164 - val_accuracy: 0.9825 - val_loss: 0.0523\n",
            "Epoch 13/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 232ms/step - accuracy: 0.9353 - loss: 0.1580 - val_accuracy: 0.9669 - val_loss: 0.0733\n",
            "Epoch 14/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.9572 - loss: 0.1149 - val_accuracy: 0.9800 - val_loss: 0.0552\n",
            "Epoch 15/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 229ms/step - accuracy: 0.9538 - loss: 0.1239 - val_accuracy: 0.9688 - val_loss: 0.0689\n",
            "Epoch 16/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.9496 - loss: 0.1215 - val_accuracy: 0.9725 - val_loss: 0.0642\n",
            "Epoch 17/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 221ms/step - accuracy: 0.9446 - loss: 0.1351 - val_accuracy: 0.9706 - val_loss: 0.0781\n",
            "Epoch 18/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9388 - loss: 0.1296 - val_accuracy: 0.9750 - val_loss: 0.0552\n",
            "Epoch 19/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 229ms/step - accuracy: 0.9491 - loss: 0.1404 - val_accuracy: 0.9737 - val_loss: 0.0660\n",
            "Epoch 20/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.9375 - loss: 0.1381 - val_accuracy: 0.9650 - val_loss: 0.0732\n",
            "Epoch 21/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 233ms/step - accuracy: 0.9445 - loss: 0.1300 - val_accuracy: 0.9712 - val_loss: 0.0723\n",
            "Epoch 22/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.9519 - loss: 0.1275 - val_accuracy: 0.9600 - val_loss: 0.0743\n",
            "Epoch 23/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 222ms/step - accuracy: 0.9550 - loss: 0.1186 - val_accuracy: 0.9650 - val_loss: 0.0817\n",
            "Epoch 24/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9511 - loss: 0.1099 - val_accuracy: 0.9750 - val_loss: 0.0624\n",
            "Epoch 25/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 229ms/step - accuracy: 0.9549 - loss: 0.1080 - val_accuracy: 0.9688 - val_loss: 0.0656\n",
            "Epoch 26/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9563 - loss: 0.1046 - val_accuracy: 0.9725 - val_loss: 0.0561\n",
            "Epoch 27/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 231ms/step - accuracy: 0.9480 - loss: 0.1219 - val_accuracy: 0.9675 - val_loss: 0.0790\n",
            "Epoch 28/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.9499 - loss: 0.1358 - val_accuracy: 0.9700 - val_loss: 0.0691\n",
            "Epoch 29/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 231ms/step - accuracy: 0.9521 - loss: 0.1264 - val_accuracy: 0.9681 - val_loss: 0.0744\n",
            "Epoch 30/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.9537 - loss: 0.1235 - val_accuracy: 0.9750 - val_loss: 0.0648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оценка"
      ],
      "metadata": {
        "id": "SjbiqhrQCQuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
        "print('Точность на контрольной выборке:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peR3wKIQCQQS",
        "outputId": "debb1bfe-3f0a-4555-f147-d38bddb7e2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.9585 - loss: 0.1318\n",
            "Точность на контрольной выборке: 0.9629999995231628\n"
          ]
        }
      ]
    }
  ]
}